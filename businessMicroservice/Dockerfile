## For Task6
FROM python:3.9-slim
# Use an official Python runtime as a parent image

ENV APP_HOME /app
# Set environment variables
ENV APP_MODE='dev'

RUN mkdir $APP_HOME
# Running the command

WORKDIR $APP_HOME
# Set the working directory
COPY . /app
# Copy the current directory represented via '.' contents into the container at /app

RUN pip install --no-cache-dir -r requirements.txt
# Install any needed packages specified in requirements.txt

EXPOSE 8801
# Make port 8801 available to the world outside this container

## Now 2 ways to run the FastAPI application:
# CMD ["./run-server.sh"] # Way 1: Run app.py when the container launches; Check the .sh file defined in repo which has the command to execute when container runs
CMD ["uvicorn", "app:app", "--host", "127.0.0.1", "--port", "8000", "--reload"] # Another way


###################################################################################################
# Note:
# A Dockerfile is a text document that contains all the commands a user could call on the command line to assemble an image. Here are the main components of a Dockerfile and explanations of when and how to use them:
# 1. FROM
# FROM <image>
# Description: Specifies the base image to use for the Docker image. This is the starting point for the build process.
# When to Use: Always, as it defines the base layer for your image.
# Example: FROM python:3.9-slim
# 2. LABEL
# LABEL key=value
# Description: Adds metadata to an image. Useful for providing information such as maintainer details or a version number.
# When to Use: When you want to include metadata in the image.
# Example: LABEL maintainer="you@example.com"
# 3. RUN
# RUN <command>
# Description: Executes a command in a new layer on top of the current image and commits the results. This is often used for installing packages.
# When to Use: When you need to install dependencies or run commands that are required to build your application.
# Example: RUN apt-get update && apt-get install -y gcc
# 4. COPY and ADD
# COPY <src> <dest>
# ADD <src> <dest>
# Description:
# COPY copies files and directories from the host file system to the image.
# ADD does the same as COPY but also supports extracting tar files and fetching files from URLs.
# When to Use: Use COPY for simple copying and ADD for additional functionalities.
# Example: COPY . /app
# 5. WORKDIR
# WORKDIR /path/to/workdir
# Description: Sets the working directory for any subsequent RUN, CMD, ENTRYPOINT, COPY, and ADD instructions.
# When to Use: When you want to set the working directory for your application.
# Example: WORKDIR /app
# 6. CMD
# CMD ["executable","param1","param2"]
# Description: Provides defaults for an executing container. There can only be one CMD instruction in a Dockerfile. If you provide multiple CMD instructions, only the last one will take effect.
# When to Use: To specify the default command to run when the container starts.
# Example: CMD ["python", "app.py"]
# 7. ENTRYPOINT
# ENTRYPOINT ["executable", "param1", "param2"]
# Description: Configures a container that will run as an executable. It allows you to configure a container to run as if it was that executable.
# When to Use: When you want to define a container with a specific executable.
# Example: ENTRYPOINT ["python"]
# 8. ENV
# ENV key=value
# Description: Sets environment variables.
# When to Use: To define environment variables that will be available in your container.
# Example: ENV APP_ENV=production
# 9. EXPOSE
# EXPOSE <port>
# Description: Informs Docker that the container listens on the specified network ports at runtime. It does not actually publish the port.
# When to Use: When your application runs on specific ports.
# Example: EXPOSE 80
# 10. VOLUME
# VOLUME ["/data"]
# Description: Creates a mount point with the specified path and marks it as holding externally mounted volumes from native host or other containers.
# When to Use: When you need to persist data generated by and used by Docker containers.
# Example: VOLUME ["/app/data"]
# 11. USER
# USER <username or UID>
# Description: Sets the user name or UID to use when running the image and for any RUN, CMD, and ENTRYPOINT instructions that follow it in the Dockerfile.
# When to Use: When you want to run the container as a non-root user for security reasons.
# Example: USER appuser
# 12. ONBUILD
# ONBUILD <instruction>
# Description: Adds a trigger instruction to the image that will be executed when the image is used as a base for another build.
# When to Use: When you want to define actions that should be taken when the image is used as a base for another build.
# Example: ONBUILD COPY . /app/src
# Here's an example Dockerfile that demonstrates the use of several components:
# # Use an official Python runtime as a parent image
# FROM python:3.9-slim
# # Set environment variables
# ENV PYTHONDONTWRITEBYTECODE=1
# ENV PYTHONUNBUFFERED=1
# # Set the working directory
# WORKDIR /app
# # Copy the current directory contents into the container at /app
# COPY . /app
# # Install any needed packages specified in requirements.txt
# RUN pip install --no-cache-dir -r requirements.txt
# # Make port 80 available to the world outside this container
# EXPOSE 80
# # Define environment variable
# ENV NAME World
# # Run app.py when the container launches
# CMD ["python", "app.py"]
# Summary
# Understanding these components helps you build efficient, manageable, and reusable Docker images. Each instruction serves a specific purpose and can be combined to define the exact environment and behavior of your Docker container.

## Good link/note: https://stackoverflow.com/questions/55108649/what-is-app-working-directory-for-a-dockerfile
## Good link/note: When you create a Docker image and publish it to Docker Hub, anyone who downloads and runs that image on their machine will have all the files and configurations that were present in the image at the time it was created. This includes: Any files and directories you copied into the image, Any software packages or dependencies you installed, Any environment variables you set, Any configurations or changes you made.
## Good link/note: Containers don't inherently have an app directory by default; it depends on how the Docker image is built and what the Dockerfile specifies. The presence of an app directory is typically a convention used by developers to organize their application files within the container. When you start a container from a base image, it will have a default set of directories typical to a Linux filesystem, depending on the base image used. Common directories include: /bin, /lib, /etc, /var, /tmp, etc. Developers can define custom directories in the Dockerfile using the WORKDIR or RUN mkdir commands. You can start a container and then use the docker exec command to run shell commands inside the container i.e then using ls to list all dirs, allowing you to explore its filesystem.

# A Docker Compose file, typically named docker-compose.yml, is used to define and manage multi-container Docker applications. It allows you to define services, networks, and volumes in a single YAML file, providing a streamlined way to manage your Docker environment. Below are the primary components of a Docker Compose file:
# 1. version
# The version key specifies the version of the Docker Compose file format. The most common versions are 2, 2.1, 3, and 3.8.
# version: '3.8'
# 2. services
# The services key is where you define the different services (containers) that make up your application. Each service can be configured with various options.
# services:
#   web:
#     image: nginx:latest
#     ports:
#       - "80:80"
#   db:
#     image: postgres:latest
#     environment:
#       POSTGRES_DB: mydatabase
#       POSTGRES_USER: user
#       POSTGRES_PASSWORD: password
# 3. networks
# The networks key allows you to define custom networks for your services. This is useful for setting up isolated network environments for your containers.
# networks:
#   mynetwork:
#     driver: bridge
# You can then assign services to these networks:
# services:
#   web:
#     image: nginx:latest
#     networks:
#       - mynetwork
#   db:
#     image: postgres:latest
#     networks:
#       - mynetwork
# 4. volumes
# The volumes key allows you to define shared storage volumes that can be used by your services. Volumes are useful for persisting data across container restarts.
# volumes:
#   myvolume:
# You can then mount these volumes in your services:
# services:
#   db:
#     image: postgres:latest
#     volumes:
#       - myvolume:/var/lib/postgresql/data
# Common Service Configuration Options
# image: Specifies the Docker image to use for the service.
# image: nginx:latest
# build: Specifies build configuration for the service. It can point to a directory with a Dockerfile or define build arguments.
# build:
#   context: ./path/to/build/context
#   dockerfile: Dockerfile
# ports: Maps ports on the host to ports in the container.
# ports:
#   - "80:80"
# environment: Defines environment variables for the service.
# environment:
#   POSTGRES_DB: mydatabase
#   POSTGRES_USER: user
#   POSTGRES_PASSWORD: password
# volumes: Mounts host directories or named volumes into the container.
# volumes:
#   - myvolume:/var/lib/postgresql/data
#   - ./localdir:/containerdir
# networks: Connects the service to one or more networks.
# networks:
#   - mynetwork
# depends_on: Specifies dependencies between services. Docker Compose will start the dependencies before starting the service.
# depends_on:
#   - db
# command: Overrides the default command for the container.
# command: python app.py
# entrypoint: Overrides the default entrypoint for the container.
# entrypoint: /app/entrypoint.sh
# Here's a full example of a docker-compose.yml file for a web application with a database:
# version: '3.8'
# services:
#   web:
#     image: nginx:latest
#     ports:
#       - "80:80"
#     networks:
#       - mynetwork
#     depends_on:
#       - app
#   app:
#     build:
#       context: ./app
#     networks:
#       - mynetwork
#     environment:
#       - DATABASE_URL=postgres://user:password@db:5432/mydatabase
#     volumes:
#       - ./app:/app
#   db:
#     image: postgres:latest
#     networks:
#       - mynetwork
#     environment:
#       POSTGRES_DB: mydatabase
#       POSTGRES_USER: user
#       POSTGRES_PASSWORD: password
#     volumes:
#       - myvolume:/var/lib/postgresql/data
# networks:
#   mynetwork:
#     driver: bridge
# volumes:
#   myvolume:
# In this example:
# web service uses the Nginx image and maps port 80.
# app service builds from a local context and connects to the same network as the web service.
# db service uses the PostgreSQL image and stores its data in a named volume.
# All services are connected to a custom bridge network.
# Environment variables are used to configure the services, and volumes are used to persist data and share files between the host and containers.

## Good link/note: Docker containers are easiest to use with stateless applications because their filesystems are ephemeral in nature. Changes made to a container’s environment are lost when the container stops, crashes, or gets replaced. You can Dockerize stateful applications such as databases and file servers by attaching volumes to your containers. Volumes provide persistent storage that’s independent of individual containers. You can reattach volumes to a different container after a failure or use them to share data between several containers simultaneously. Volumes are a mechanism for storing data outside containers. All volumes are managed by Docker and stored in a dedicated directory on your host, usually /var/lib/docker/volumes for Linux systems. Volumes are mounted to filesystem paths in your containers. When containers write to a path beneath a volume mount point, the changes will be applied to the volume instead of the container’s writable image layer. The written data will still be available if the container stops – as the volume’s stored separately on your host, it can be remounted to another container or accessed directly using manual tools. Bind mounts are another way to give containers access to files and folders on your host. They directly mount a host directory into your container. Any changes made to the directory will be reflected on both sides of the mount, whether the modification originates from the host or within the container. Bind mounts are best used for ad-hoc storage on a short-term basis. They’re convenient in development workflows. For example: bind mounting your working directory into a container automatically synchronizes your source code files, allowing you to immediately test changes without rebuilding your Docker image. Volumes are a better solution when you’re providing permanent storage to operational containers. Because they’re managed by Docker, you don’t need to manually maintain directories on your host. There’s less chance of data being accidentally modified and no dependency on a particular folder structure. Volume drivers also offer increased performance and the possibility of writing changes directly to remote locations.

